import streamlit as st
import pandas as pd
import numpy as np
import re
import os
import joblib
from gensim.models import Word2Vec

# --- 1. åŠ è½½æ‰€æœ‰ä¿å­˜å¥½çš„æ„ä»¶ ---
@st.cache_resource
def load_artifacts():
    artifact_dir = "model_artifacts"
    try:
        median_values = joblib.load(os.path.join(artifact_dir, 'median_values.pkl'))
        capping_limits = joblib.load(os.path.join(artifact_dir, 'capping_limits.pkl'))
        scaler = joblib.load(os.path.join(artifact_dir, 'scaler.pkl'))
        w2v_model = Word2Vec.load(os.path.join(artifact_dir, "word2vec.model"))
        final_model = joblib.load(os.path.join(artifact_dir, 'best_multilabel_model.pkl'))
        final_feature_order = joblib.load(os.path.join(artifact_dir, 'final_feature_order.pkl'))
        
        all_cols_from_training = joblib.load(os.path.join(artifact_dir, 'feature_columns.pkl'))
        place_title_options = sorted([col.replace('place_title_', '') for col in all_cols_from_training if col.startswith('place_title_')])
        class_both_options = sorted([col.replace('class_both_', '') for col in all_cols_from_training if col.startswith('class_both_')])
        
        return median_values, capping_limits, scaler, w2v_model, final_model, final_feature_order, place_title_options, class_both_options
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        st.error(f"è¯·ç¡®ä¿ '{artifact_dir}' æ–‡ä»¶å¤¹å­˜åœ¨ï¼Œå¹¶ä¸”åŒ…å«äº†æ‰€æœ‰å¿…è¦çš„ .pkl, .model æ–‡ä»¶ã€‚")
        return [None] * 8

# åŠ è½½æ¨¡å‹å’Œå·¥å…·
median_values, capping_limits, scaler, w2v_model, final_model, final_feature_order, place_title_options, class_both_options = load_artifacts()

# --- 2. å®šä¹‰å®Œæ•´çš„é¢„å¤„ç†å‡½æ•° ---
def preprocess_input(user_input):
    df = pd.DataFrame([user_input])

    numeric_cols = ['reviewerNumberOfReviews', 'reviewsCount', 'stars', 'place_totalScore']
    outlier_cols = ['reviewerNumberOfReviews', 'reviewsCount']
    df[numeric_cols] = df[numeric_cols].fillna(pd.Series(median_values))
    for col in outlier_cols:
        df.loc[df[col] > capping_limits[col], col] = capping_limits[col]
    df[numeric_cols] = scaler.transform(df[numeric_cols])

    df['isLocalGuide'] = df['isLocalGuide'].astype(int)
    
    def preprocess_text_for_w2v(text):
        if not isinstance(text, str): return []
        text = text.lower()
        text = re.sub(r'[^a-z\s]', '', text)
        return text.split()
    
    def get_sentence_vector(tokens, model):
        vectors = [model.wv[word] for word in tokens if word in model.wv]
        if len(vectors) == 0: return np.zeros(model.vector_size)
        return np.mean(vectors, axis=0)

    tokens = preprocess_text_for_w2v(df['review_text'].iloc[0])
    sentence_vector = get_sentence_vector(tokens, w2v_model)
    w2v_df = pd.DataFrame([sentence_vector], columns=[f'w2v_{i}' for i in range(100)])

    df_processed = df.drop(columns=['class_both', 'place_title', 'review_text'])
    df_processed = pd.concat([df_processed, w2v_df], axis=1)

    final_input_df = pd.DataFrame(columns=final_feature_order)
    final_input_df = pd.concat([final_input_df, df_processed], axis=0).fillna(0)

    class_col_name_cleaned = 'class_both_' + re.sub(r'[^A-Za-z0-9_]+', '_', user_input['class_both'])
    if class_col_name_cleaned in final_input_df.columns:
        final_input_df[class_col_name_cleaned] = 1
        
    place_col_name_cleaned = 'place_title_' + re.sub(r'[^A-Za-z0-9_]+', '_', user_input['place_title'])
    if place_col_name_cleaned in final_input_df.columns:
        final_input_df[place_col_name_cleaned] = 1
        
    return final_input_df[final_feature_order]

# --- 3. Streamlit ç½‘é¡µç•Œé¢ ---
st.set_page_config(page_title="è¿è§„è¯„è®ºæ£€æµ‹å™¨", page_icon="ğŸš¨", layout="wide")
st.title("ğŸš¨ Google Maps è¿è§„è¯„è®ºæ™ºèƒ½æ£€æµ‹å™¨ (Demo)")
st.markdown("è¾“å…¥è¯„è®ºåŠç›¸å…³ä¿¡æ¯ï¼Œåå°å°†è°ƒç”¨è®­ç»ƒå¥½çš„ **LightGBM** æ¨¡å‹è¿›è¡Œå®æ—¶æ£€æµ‹ã€‚")

if final_model:
    # --- âœ¨ å…³é”®ä¿®æ­£ï¼šå®šä¹‰å†³ç­–é˜ˆå€¼ ---
    # ä¸ºäº†è¿½æ±‚é«˜å¬å›ç‡ï¼Œæˆ‘ä»¬å°†é˜ˆå€¼è®¾ä¸º0.3
    THRESHOLDS = {
        'if_ad': 0.3,
        'if_irrelevant': 0.3,
        'if_not_experience': 0.3
    }
    st.info(f"å½“å‰æ£€æµ‹ç­–ç•¥ï¼šé«˜å¬å›ç‡æ¨¡å¼ï¼ˆé˜ˆå€¼ = {list(THRESHOLDS.values())[0]}ï¼‰ã€‚ä»»ä½•è¿è§„å¯èƒ½æ€§è¶…è¿‡æ­¤å€¼çš„è¯„è®ºéƒ½å°†è¢«æ ‡è®°ä¸ºè¿è§„ã€‚")
    st.markdown("---")

    # åˆ›å»ºè¾“å…¥ç•Œé¢
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ğŸ“ è¯„è®ºå†…å®¹")
        review_text_input = st.text_area("Review Text", "I didn't came here before but it sounds good!", height=150)
        stars_input = st.slider("æ˜Ÿçº§ (Stars)", 1, 5, 2)
        is_local_guide_input = st.checkbox("æ˜¯æœ¬åœ°å‘å¯¼ (Is Local Guide)?", False)

    with col2:
        st.subheader("ğŸ‘¤ è¯„è®ºè€…ä¸åœ°ç‚¹ä¿¡æ¯")
        reviewer_reviews_input = st.number_input("è¯„è®ºè€…å†å²è¯„è®ºæ•°", min_value=0, value=1)
        place_reviews_input = st.number_input("åœ°ç‚¹æ€»è¯„è®ºæ•°", min_value=0, value=50)
        place_score_input = st.slider("åœ°ç‚¹æ€»è¯„åˆ†", 1.0, 5.0, 3.5, 0.1)
        place_title_input = st.selectbox("åœ°ç‚¹åç§°", options=place_title_options, index=0 if not place_title_options else min(5, len(place_title_options)-1))
        class_both_input = st.selectbox("åœ°ç‚¹ç±»åˆ«", options=class_both_options, index=0 if not class_both_options else min(5, len(class_both_options)-1))

    if st.button("å¼€å§‹æ£€æµ‹", type="primary", use_container_width=True):
        if not review_text_input:
            st.warning("è¯·è¾“å…¥è¯„è®ºå†…å®¹ä»¥è¿›è¡Œæ£€æµ‹ï¼")
        else:
            user_input_data = {
                'isLocalGuide': is_local_guide_input,
                'reviewerNumberOfReviews': reviewer_reviews_input,
                'reviewsCount': place_reviews_input,
                'stars': stars_input,
                'review_text': review_text_input,
                'place_title': place_title_input,
                'place_totalScore': place_score_input,
                'class_both': class_both_input
            }
            
            with st.spinner("æ¨¡å‹æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
                features = preprocess_input(user_input_data)
                prediction_proba = final_model.predict_proba(features)

            st.subheader("ğŸ” æ£€æµ‹ç»“æœ")
            label_columns = ['if_ad', 'if_irrelevant', 'if_not_experience']
            results_cols = st.columns(len(label_columns))
            violation_found_count = 0
            
            for i, label in enumerate(label_columns):
                with results_cols[i]:
                    prob = prediction_proba[i][0, 1]
                    threshold = THRESHOLDS.get(label, 0.5) # ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„é˜ˆå€¼
                    
                    if prob >= threshold:
                        violation_found_count += 1
                        st.metric(label=label, value="æ£€æµ‹åˆ°è¿è§„", delta=f"{prob:.1%} å¯èƒ½æ€§", delta_color="inverse")
                    else:
                        st.metric(label=label, value="åˆè§„", delta=f"{prob:.1%} å¯èƒ½æ€§", delta_color="off")

            st.markdown("---")
            if violation_found_count > 0:
                st.warning(f"**ç»¼åˆç»“è®ºï¼š** è¯¥è¯„è®ºæ£€æµ‹åˆ° **{violation_found_count}** é¡¹æ½œåœ¨è¿è§„ï¼Œå»ºè®®è¿›è¡Œäººå·¥å®¡æ ¸ã€‚")
            else:
                st.balloons()
                st.success("**ç»¼åˆç»“è®ºï¼š** ğŸ‰ è¯„è®ºå†…å®¹åˆè§„ã€‚")
else:
    st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·å…ˆè¿è¡Œ `train_and_save_artifacts.py`ã€‚")